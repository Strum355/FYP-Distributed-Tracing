\documentclass[12pt,pdftex,titlepage]{report}

\author{Noah Santschi-Cooney\\\\\\\small{Jason Quinlan}\\\\\small{University College Cork}}
\title{\textbf{Alternative Visualisation of Distributed Tracing data in a complex, large-scale distributed system}}
\date{\vfill\small{3rd April 2020}}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\pagenumbering{roman}

\usepackage{graphicx}
\graphicspath{{./assets/}}

\PassOptionsToPackage{hyphens}{url}\usepackage{url}
\usepackage{microtype}

\makeatletter
\def\@makechapterhead#1{%
  \vspace*{20\p@}% <----------------- Space from top of page to Chapter #
  {\parindent \z@ \raggedright \normalfont
    \ifnum \c@secnumdepth >\m@ne
        \huge\bfseries \thechapter.\ % <-- Chapter # (without "Chapter")
    \fi
    \interlinepenalty\@M
    #1\par\nobreak% <------------------ Chapter title
    \vskip 20\p@% <------------------ Space between chapter title and first paragraph
  }}
\makeatother

\begin{document}
    \maketitle    

    \chapter*{Abstract}
        \addcontentsline{toc}{chapter}{Abstract}
        Modern Internet services are often implemented as complex, large-scale distributed systems. These applications are constructed from collections 
        of software modules that could span many thousands of machines across multiple physical facilities. With the rise of modern Micro-service and 
        Service-Oriented designs, traditional tooling used to monitor application behaviour is no longer viable, especially at scale. 
        
        To understanding the flow and life cycle of a unit of work performed in multiple pieces across various components in a distributed system, the concept of 
        Distributed Tracing was born. Distributed Tracing was first introduced to the mainstream world in 2010 after the publication of Google’s Dapper
        paper. Since then, various vendors have come out with their own Dapper-inspired services, most of them based off flame or timeline graphs. 
        
        The goal of this project is dual-faceted:
        \begin{itemize}
            \item Explore and research possible alternative uses and visualisation methods utilising data collected from distributed tracing clients.
            \item Implement one or more of the proposed alternatives.
        \end{itemize}

    \chapter*{Declaration of Originality}
        \addcontentsline{toc}{chapter}{Declaration of Originality}
        In signing this declaration, you are confirming, in writing, that the submitted work
        is entirely your own original work, except where clearly attributed otherwise, and
        that it has not been submitted partly or wholly for any other educational award. I
        hereby declare that:
        \begin{itemize}
            \item this is all my own work, unless clearly indicated otherwise, with full and proper accreditation;  
            \item with respect to my own work: none of it has been submitted at any educational institution contributing in any way to an educational award;
            \item with respect to another’s work: all text, diagrams, code, or ideas, whether verbatim, paraphrased or otherwise modified or adapted, 
            have been duly attributed to the source in a scholarly manner, whether from books, papers, lecture notes or any other student’s work, whether
            published or unpublished, electronically or in print.
        \end{itemize}

    \chapter*{Acknowledgements}
        \addcontentsline{toc}{chapter}{Acknowledgements}
        
    \tableofcontents

    \chapter{Introduction}
    \pagenumbering{arabic}
    \setcounter{page}{1}
        \section{Problem}
            Within the last decade, the way modern applications are being built and deployed has changed dramatically. With the shift from collocation to cloud computing,
            virtual machines to containerization technologies, monoliths to micro-services and beyond, software developers have been able to adjust to 
            the monotonical increase in internet traffic, shipping highly scalable, efficient and reliable software that meets the ever-demanding needs of their customers
            with the slew of emerging technologies.

            While this shift has undoubtedly solved many issues with regards to scaling services in terms of both maintainability as feature sets increase and in keeping up
            with an every larger number of online users, it has introduced a whole new suite of problems that needed to be addressed in terms of reliability and application 
            monitoring. With the splitting of monolithic applications into micro-services, the failure points are extended to issues in the network, including but not limited
            to network congestion, DNS resolution errors etc. Developers are ever more inclined to code failure resilience into their applications, falling back gracefully in 
            apprehension of unforeseeable failures.

            As these new distributed system architectures evolved and became ever more widespread, traditional application monitoring tools consistently fell short of providing
            developers and systems operators with the means to gain introspection into systems and their failures in production scenarios. Traditional monolithic systems often
            utilized logging and metrics to gain introspection into the application and for alerting on rules respectively. For such systems, these process-scoped measures often 
            provided good insight into a system, correlating logs on their thread identifier/name as each thread would handle a single request sequentially. As these systems 
            adopted asynchronous execution models, where a request's lifetime may not be confined to a single thread, the previous approach no longer works, making observing
            the behaviour of such systems very difficult unless developers annotated logs with request-scoped identifiers. The final evolution of concurrency in application systems is
            commonly referred to as \textit{distributed concurrency}. This is often associated with micro-services, in which a request is no longer constrained to being executed
            in a single process, but may span multiple processes and even servers. Figure~\ref{fig:concurrency} highlights this evolution, from simple, single threaded applications,
            through to micro-service-like architectures.

            \begin{figure}[htb!]
                \centering
                \includegraphics{concurrency.png}
                \caption{Evolution of concurrent systems.}
                \label{fig:concurrency}
            \end{figure}

        \section{Distributed Tracing}
            As traditional tooling is not designed to accommodate for this distributed concurrency system, new methodologies were needed to regain observability into the systems.
            Observing single systems individually, as was done with traditional tooling, no longer painted the full picture of a request as it travels through multiple system 
            components. There was a need for a system to reconstruct a request from a series of event streams from each component involved in the request, building a causality
            graph from a request-centric point of view. 

    \chapter{Background}
        \section{Dapper}
        Released in April 2010, Google published a paper describing the design decisions behind an in-house implementation 
        of distributed tracing, named Dapper. It is commonly believed that this paper describes the common ancestor to 
        many tools that implement a form of distributed tracing.

        The Dapper paper introduces some of the core primitives that underpin modern day standards. Most notable are the concepts
        of a directed acyclic graph (DAG) called a \textit{trace tree} and its nodes, which are referred to as \textit{spans}. 
        The trace tree forms a relationship between spans, not unakin to a tree of stack frames that may be generated by
        gathering stack frames over time, albeit generally at a much higher level than at the level of individual subroutine calls. 

        Figure~\ref{fig:dappertrace} illustrates a trace tree with five spans. Each span is shown to contain 3 specific pieces of
        metadata alongside the start and end timestamps necessarily to reconstruct the temporal relationships: a human-readable
        \textit{span name}, an integer \textit{span ID} and an integer \textit{parent ID}. The latter two
        data points are used to reconstruct the relationship between individual spans. A span without a parent ID becomes the 
        \textit{root span} of a trace tree. Not shown is another important but, as of right now, not relevant piece of metadata, the 
        \textit{trace ID}, which is common amongst all spans within a single trace tree.

        \begin{figure}[htb!]
            \centering
            \includegraphics[scale=0.9]{dappertrace}
            \caption{The relationships between traces in a trace tree.}
            \label{fig:dappertrace}
        \end{figure}

        As described thus far, Dapper trace trees allow for a detailed view of the relationships of distributed systems within
        Google. When using this data for debugging or performance analysis, it can often be convenient or even necessary to 
        have additional context surrounding a trace tree or its individual spans. As such, the paper describes a simple API 
        through which application developers can provide a combination of two types of annotations: timestamped textual annotations
        and key-value, allowing for defining arbitrary equivalence classes between traces which can be operated upon in the analysis
        tools.

        \section{OpenTracing}
        OpenTracing\cite{opentracing} project's inception came about in October 2015, it has since become a project under the 
        Cloud Native Computing Foundation in 2016, created to standardize a set of vendor neutral and programming language agnostic
        application programming interfaces (APIs) for instrumenting code for distributed tracing. Heavily inspired by the Dapper
        paper, it borrows many of the nouns and verbs outlined in the Dapper paper, including \textit{traces} and \textit{spans}.
        Dapper's timestamped annotations are referred to as \textit{logs} in the OpenTracing specification, while the key-value pairs
        are named \textit{tags}. 

        The OpenTracing API also specifies how a trace cross process boundaries, so that spans created in different processes can be
        associated with a common trace tree. This was named the \textit{span context} and at it's most basic level contains the 
        overlying trace ID as well as the current span ID. With this, new spans generated across process boundaries have the ability to
        to specify their parent span as well as their common trace, without propagating an entire span, which may prove costly as more
        tags and logs are attached to a span.
    
    
    \chapter{Conclusion}
    
    \begin{thebibliography}{69}
        \addcontentsline{toc}{chapter}{Bibliography}    

        \bibitem{dapper}
        Benjamin H. Sigelman, Luiz André Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Saul Jaspan, Chandan Shanbhag, \\
        \textit{Dapper, a Large-Scale Distributed Systems Tracing Infrastructure.} \\
        Google, Inc. 2010 \\
        \url{https://research.google/pubs/pub36356/}

        \bibitem{mbta}
        Mike Barry, Brian Card, \textit{Visualizing MBTA Data.}
        10th June 2014 \\
        \url{https://mbtaviz.github.io/}

        \bibitem{opentracing}
        Benjamin H. Sigelman (co creator), \textit{The OpenTracing project.}
        October 2015 \\
        \url{https://opentracing.io/}
    \end{thebibliography}
\end{document}